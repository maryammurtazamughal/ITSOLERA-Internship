{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoilmKQy6cAS"
      },
      "outputs": [],
      "source": [
        "!pip install pytesseract\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install tesseract-ocr\n",
        "!sudo apt-get install libtesseract-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV1-ElXmU0ta"
      },
      "outputs": [],
      "source": [
        "! pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8r1I5otU8I7"
      },
      "outputs": [],
      "source": [
        "! pip install pdf2image -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHbnbiXbVBS5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pdf2image import convert_from_path\n",
        "from docx import Document\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c05Q8nTu6z24"
      },
      "outputs": [],
      "source": [
        "import pytesseract\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PDQqvgOWq3q"
      },
      "outputs": [],
      "source": [
        "!apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqG37lAA4jg2",
        "outputId": "2be50ce0-eb82-4b04-9892-e9b97dc29ae9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    text = \"\"\n",
        "    images = convert_from_path(file_path)\n",
        "    for image in images:\n",
        "        text += pytesseract.image_to_string(image)\n",
        "    return text\n",
        "\n",
        "def extract_text_from_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    text = []\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text.append(paragraph.text)\n",
        "    return '\\n'.join(text)\n",
        "\n",
        "def extract_text_from_doc(file_path):\n",
        "    output_file_path = file_path + \".txt\"\n",
        "    subprocess.run(['antiword', file_path, '-m', 'UTF-8', '-t', output_file_path])\n",
        "    with open(output_file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def extract_text(file_path):\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    if ext == '.pdf':\n",
        "        return extract_text_from_pdf(file_path)\n",
        "    elif ext == '.docx':\n",
        "        return extract_text_from_docx(file_path)\n",
        "    elif ext == '.doc':\n",
        "        return extract_text_from_doc(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "# Example usage:\n",
        "# file_path = \"D:/new/itsolera-assignment-1/CV.pdf\"\n",
        "\n",
        "file_path = input(\"enter your file\")\n",
        "# Change to your file path\n",
        "text = extract_text(file_path)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8L3nocnGRA4",
        "outputId": "cc368295-e6c4-4613-f4a9-8525229d5b24"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import re\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = ' '.join(text.split())\n",
        "    text = re.sub(r'[\\r\\n]+', '\\n', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "def split_section_content(content):\n",
        "    \"\"\" Split section content using common delimiters and heuristics. \"\"\"\n",
        "    return re.split(r'\\s*[,;]\\s*|\\n|(?<=\\S)\\s{2,}(?=\\S)', content)\n",
        "\n",
        "def extract_info_from_cv(raw_text):\n",
        "    text = preprocess_text(raw_text)\n",
        "    doc = nlp(text)\n",
        "\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "\n",
        "    patterns = {\n",
        "        'SKILLS_HEADER': [\n",
        "            [{'LOWER': 'skills'}, {'IS_PUNCT': True, 'OP': '?'}, {'IS_SPACE': True, 'OP': '*'}],\n",
        "            [{'LOWER': 'abilities'}, {'IS_PUNCT': True, 'OP': '?'}, {'IS_SPACE': True, 'OP': '*'}],\n",
        "            [{'LOWER': 'expertise'}, {'IS_PUNCT': True, 'OP': '?'}, {'IS_SPACE': True, 'OP': '*'}]\n",
        "        ],\n",
        "        'CERTIFICATES_HEADER': [\n",
        "            [{'LOWER': 'certifications'}, {'IS_PUNCT': True, 'OP': '?'}, {'IS_SPACE': True, 'OP': '*'}],\n",
        "            [{'LOWER': 'certificates'}, {'IS_PUNCT': True, 'OP': '?'}, {'IS_SPACE': True, 'OP': '*'}],\n",
        "            [{'LOWER': 'accreditations'}, {'IS_PUNCT': True, 'OP': '?'}, {'IS_SPACE': True, 'OP': '*'}]\n",
        "        ],\n",
        "        'PROJECTS_HEADER': [\n",
        "            [{'LOWER': 'projects'}, {'IS_PUNCT': True, 'OP': '?'}, {'IS_SPACE': True, 'OP': '*'}],\n",
        "            [{'LOWER': 'works'}, {'IS_PUNCT': True, 'OP': '?'}, {'IS_SPACE': True, 'OP': '*'}],\n",
        "            [{'LOWER': 'assignments'}, {'IS_PUNCT': True, 'OP': '?'}, {'IS_SPACE': True, 'OP': '*'}]\n",
        "        ],\n",
        "        'EXPERIENCE_HEADER': [\n",
        "            [{'LOWER': 'experience'}, {'IS_PUNCT': True, 'OP': '?'}, {'IS_SPACE': True, 'OP': '*'}],\n",
        "            [{'LOWER': 'employment'}, {'IS_PUNCT': True, 'OP': '?'}, {'IS_SPACE': True, 'OP': '*'}],\n",
        "            [{'LOWER': 'work history'}, {'IS_PUNCT': True, 'OP': '?'}, {'IS_SPACE': True, 'OP': '*'}]\n",
        "        ],\n",
        "        'EDUCATION_HEADER': [\n",
        "            [{'LOWER': 'education'}, {'IS_PUNCT': True, 'OP': '?'}, {'IS_SPACE': True, 'OP': '*'}],\n",
        "            [{'LOWER': 'academic background'}, {'IS_PUNCT': True, 'OP': '?'}, {'IS_SPACE': True, 'OP': '*'}],\n",
        "            [{'LOWER': 'qualifications'}, {'IS_PUNCT': True, 'OP': '?'}, {'IS_SPACE': True, 'OP': '*'}]\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    for label, patterns_list in patterns.items():\n",
        "        for pattern in patterns_list:\n",
        "            matcher.add(label, [pattern])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    extracted_info = {'name': '', 'email': '', 'phone': '', 'skills': [], 'certificates': [], 'projects': [], 'experience': [], 'education': []}\n",
        "\n",
        "    # Extract the name (first two proper nouns)\n",
        "    proper_nouns = [token.text for token in doc if token.pos_ == 'PROPN']\n",
        "    if len(proper_nouns) >= 2:\n",
        "        extracted_info['name'] = ' '.join(proper_nouns[:2])\n",
        "\n",
        "    # Extract email and phone number using regex\n",
        "    email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
        "    phone_pattern = re.compile(r'\\b(?:\\d{10}|\\d{3}[-.\\s]\\d{3}[-.\\s]\\d{4}|\\d{4}[-.\\s]?\\d{7}|\\d{11})\\b')\n",
        "\n",
        "    email_match = email_pattern.search(text)\n",
        "    phone_match = phone_pattern.search(text)\n",
        "\n",
        "    if email_match:\n",
        "        extracted_info['email'] = email_match.group().strip()\n",
        "    if phone_match:\n",
        "        extracted_info['phone'] = phone_match.group().strip()\n",
        "\n",
        "    # Extract sections based on headers\n",
        "    section_starts = {}\n",
        "    for match_id, start, end in matches:\n",
        "        label = nlp.vocab.strings[match_id]\n",
        "        section_starts[label] = start\n",
        "\n",
        "    sorted_sections = sorted(section_starts.items(), key=lambda x: x[1])\n",
        "\n",
        "    sections = {\n",
        "        'skills': [],\n",
        "        'certificates': [],\n",
        "        'projects': [],\n",
        "        'experience': [],\n",
        "        'education': []\n",
        "    }\n",
        "\n",
        "    for i, (section, start) in enumerate(sorted_sections):\n",
        "        end = sorted_sections[i + 1][1] if i + 1 < len(sorted_sections) else len(doc)\n",
        "        content = doc[start:end].text.strip()\n",
        "        items = split_section_content(content)\n",
        "        if section == 'SKILLS_HEADER':\n",
        "            sections['skills'] = [item for item in items if item.strip() and not re.match(r'^(Certifications|Projects|Experience|Education):', item, re.IGNORECASE)]\n",
        "        elif section == 'CERTIFICATES_HEADER':\n",
        "            sections['certificates'] = [item for item in items if item.strip() and not re.match(r'^(Projects|Experience|Education):', item, re.IGNORECASE)]\n",
        "        elif section == 'PROJECTS_HEADER':\n",
        "            sections['projects'] = [item for item in items if item.strip() and not re.match(r'^(Experience|Education):', item, re.IGNORECASE)]\n",
        "        elif section == 'EXPERIENCE_HEADER':\n",
        "            sections['experience'] = [item for item in items if item.strip() and not re.match(r'^Education:', item, re.IGNORECASE)]\n",
        "        elif section == 'EDUCATION_HEADER':\n",
        "            sections['education'] = [item for item in items if item.strip() and not re.match(r'^(Skills|Certifications|Projects|Experience):', item, re.IGNORECASE)]\n",
        "\n",
        "    # Remove section headers from the first element of each section\n",
        "    for key in sections:\n",
        "        if sections[key] and re.match(r'^(Skills|Certifications|Projects|Experience|Education):', sections[key][0], re.IGNORECASE):\n",
        "            sections[key][0] = re.sub(r'^(Skills|Certifications|Projects|Experience|Education):\\s*', '', sections[key][0], flags=re.IGNORECASE)\n",
        "\n",
        "    extracted_info['skills'] = sections['skills']\n",
        "    extracted_info['certificates'] = sections['certificates']\n",
        "    extracted_info['projects'] = sections['projects']\n",
        "    extracted_info['experience'] = sections['experience']\n",
        "    extracted_info['education'] = sections['education']\n",
        "\n",
        "    return extracted_info\n",
        "\n",
        "# Sample OCR extracted text from a CV\n",
        "cv_text = \"\"\"\n",
        "Maryam Murtaza\n",
        "AI Engineer\n",
        "\n",
        "Email: maryammurtazamughal@gmail.com\n",
        "Phone: 0349-2261022\n",
        "\n",
        "Skills: Python, Machine Learning,Computer Vision, Data Analysis\n",
        "\n",
        "Projects:\n",
        "Developed an end-to-end Diabetic Retinopathy Detection Software using Computer Vision\n",
        "Created a web application for real-time data visualization\n",
        "\n",
        "Experience:\n",
        "Software Engineer at ABC Corp (2018-2022)\n",
        "Data Analyst at XYZ Ltd (2015-2018)\n",
        "\n",
        "Education:\n",
        "BSc in Computer Science, XYZ University (2011-2015)\n",
        "\"\"\"\n",
        "\n",
        "# Extract information from the sample text\n",
        "extracted_info = extract_info_from_cv(cv_text)\n",
        "print(extracted_info)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
